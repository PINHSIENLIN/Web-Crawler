---
title: "Web Crawler"
author: "Alvin, Lin"
date: "`r Sys.Date()`"
date-format: full
format:
   html:
     code-fold: show
     theme: flatly
     embed-resources: true
toc: true
toc-depth: 3
toc-location: left
execute:
  warning: false 
  keep-md: true
---

```{r}
#| echo: false
#| message: false
library(reticulate)
library(tidyverse)
library(rvest)
library(reactable)
library(gt)
```

# Python Web Crawler
```{python}
#| column: page
# Get url from PTT movies
import urllib.request as req
url = "https://www.ptt.cc/bbs/movie/index.html"
# Request with headers information
request = req.Request(url, headers = {
  # Headers should be a dictionary
  "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/110.0"
})
with req.urlopen(request) as response:
  data = response.read().decode("utf-8")
```

```{python}
#| column: page
# header
import bs4
root = bs4.BeautifulSoup(data,"html.parser")
titles = root.find_all("div", class_= "title")
for title in titles:
  if title.a != None:
    print(title.a.string)
```

## Cookies
```{python}
#| column: page
# Get url from PTT Gossiping
import urllib.request as req
def getData(url):

    # Request with headers information
    request = req.Request(url, headers = {
      # Headers should be a dictionary
      ## Add cookie in the header
      "cookie":"over18=1",
      "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/110.0"
    })
    with req.urlopen(request) as response:
      data = response.read().decode("utf-8")
    
    import bs4
    root = bs4.BeautifulSoup(data,"html.parser")
    titles = root.find_all("div", class_= "title")
    for title in titles:
      if title.a != None:
        print(title.a.string)
        
    nextLink = root.find("a", string = "‹ 上頁")
    return nextLink["href"]
```

```{python}
#| column: page
pageURL = "https://www.ptt.cc/bbs/Gossiping/index.html"
count = 0
while count < 3:
  pageURL = "https://www.ptt.cc" + getData(pageURL)
  count +=1
```

## AJAX(Asynchronous JavaScript and XML) (XHR)
```{python}
#| column: page
# Get url from PTT movies
import urllib.request as req
url = "https://medium.com/_/api/home-feed"
# Request with headers information
request = req.Request(url, headers = {
  # Headers should be a dictionary
  "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/110.0"
})
with req.urlopen(request) as response:
  data = response.read().decode("utf-8")
```

```{python}
#| column: page
# Json Format
import json
data = data.replace("])}while(1);</x>","")
data = json.loads(data) 

posts = data["payload"]["references"]["Post"]
for key in posts:
  post = posts[key]
  print(post["title"])
```

## Request Data(Request Payload Request Body)
```{python}
#| column: page
# Get url from Medium
import urllib.request as req
import json
url = "https://medium.com/_/graphql"
# Request with headers information
requestData = {"operationName":"CuratedHomeFeedModuleQuery","variables":{"paging":{"from":"10","limit":10}},"query":"query CuratedHomeFeedModuleQuery($paging: PagingOptions!) {\n  staffPicksFeed(input: {paging: $paging}) {\n    items {\n      ...CuratedHomeFeedItems_homeFeedItems\n      __typename\n    }\n    pagingInfo {\n      next {\n        page\n        limit\n        from\n        __typename\n      }\n      __typename\n    }\n    __typename\n  }\n}\n\nfragment CuratedHomeFeedItems_homeFeedItems on HomeFeedItem {\n  __typename\n  post {\n    id\n    title\n    ...HomeFeedItem_post\n    __typename\n  }\n}\n\nfragment HomeFeedItem_post on Post {\n  __typename\n  id\n  title\n  firstPublishedAt\n  mediumUrl\n  collection {\n    id\n    name\n    domain\n    logo {\n      id\n      __typename\n    }\n    __typename\n  }\n  creator {\n    id\n    name\n    username\n    imageId\n    mediumMemberAt\n    __typename\n  }\n  previewImage {\n    id\n    __typename\n  }\n  previewContent {\n    subtitle\n    __typename\n  }\n  readingTime\n  tags {\n    ...TopicPill_tag\n    __typename\n  }\n  ...BookmarkButton_post\n  ...OverflowMenuButtonWithNegativeSignal_post\n  ...PostPresentationTracker_post\n  ...PostPreviewAvatar_post\n  ...Star_post\n}\n\nfragment TopicPill_tag on Tag {\n  __typename\n  id\n  displayTitle\n  normalizedTagSlug\n}\n\nfragment BookmarkButton_post on Post {\n  visibility\n  ...SusiClickable_post\n  ...AddToCatalogBookmarkButton_post\n  __typename\n  id\n}\n\nfragment SusiClickable_post on Post {\n  id\n  mediumUrl\n  ...SusiContainer_post\n  __typename\n}\n\nfragment SusiContainer_post on Post {\n  id\n  __typename\n}\n\nfragment AddToCatalogBookmarkButton_post on Post {\n  ...AddToCatalogBase_post\n  __typename\n  id\n}\n\nfragment AddToCatalogBase_post on Post {\n  id\n  __typename\n}\n\nfragment OverflowMenuButtonWithNegativeSignal_post on Post {\n  id\n  ...OverflowMenuWithNegativeSignal_post\n  __typename\n}\n\nfragment OverflowMenuWithNegativeSignal_post on Post {\n  id\n  creator {\n    id\n    __typename\n  }\n  collection {\n    id\n    __typename\n  }\n  ...OverflowMenuItemUndoClaps_post\n  __typename\n}\n\nfragment OverflowMenuItemUndoClaps_post on Post {\n  id\n  clapCount\n  ...ClapMutation_post\n  __typename\n}\n\nfragment ClapMutation_post on Post {\n  __typename\n  id\n  clapCount\n  ...MultiVoteCount_post\n}\n\nfragment MultiVoteCount_post on Post {\n  id\n  ...PostVotersNetwork_post\n  __typename\n}\n\nfragment PostVotersNetwork_post on Post {\n  id\n  voterCount\n  recommenders {\n    name\n    __typename\n  }\n  __typename\n}\n\nfragment PostPresentationTracker_post on Post {\n  id\n  visibility\n  previewContent {\n    isFullContent\n    __typename\n  }\n  collection {\n    id\n    slug\n    __typename\n  }\n  __typename\n}\n\nfragment PostPreviewAvatar_post on Post {\n  __typename\n  id\n  collection {\n    id\n    name\n    ...CollectionAvatar_collection\n    __typename\n  }\n  creator {\n    id\n    username\n    name\n    ...UserAvatar_user\n    ...userUrl_user\n    ...useIsVerifiedBookAuthor_user\n    __typename\n  }\n}\n\nfragment CollectionAvatar_collection on Collection {\n  name\n  avatar {\n    id\n    __typename\n  }\n  ...collectionUrl_collection\n  __typename\n  id\n}\n\nfragment collectionUrl_collection on Collection {\n  id\n  domain\n  slug\n  __typename\n}\n\nfragment UserAvatar_user on User {\n  __typename\n  id\n  imageId\n  mediumMemberAt\n  name\n  username\n  ...userUrl_user\n}\n\nfragment userUrl_user on User {\n  __typename\n  id\n  customDomainState {\n    live {\n      domain\n      __typename\n    }\n    __typename\n  }\n  hasSubdomain\n  username\n}\n\nfragment useIsVerifiedBookAuthor_user on User {\n  verifications {\n    isBookAuthor\n    __typename\n  }\n  __typename\n  id\n}\n\nfragment Star_post on Post {\n  id\n  creator {\n    id\n    __typename\n  }\n  __typename\n}\n"}
request = req.Request(url, 
                      headers = {
                          "Content-Type":"application/json",
                        # Headers should be a dictionary
                          "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/110.0"},
                        # request Data
                     data = json.dumps(requestData).encode("utf-8"))
with req.urlopen(request) as response:
  result = response.read().decode("utf-8")
  # String to Dictionary with json loads
result = json.loads(result)
# List contains all title
items = result["data"]["staffPicksFeed"]["items"]
for item in items:
  print(item["post"]["title"])
# print(result["data"]["staffPicksFeed"]["items"][0]["post"]["title"])
```

## Selenium
```{python}
#| column: page
#| eval: false
#| message: false
# import Selenium module
from selenium import webdriver
from msedge.selenium_tools import Edge
# Path to Driver
driver = Edge(executable_path = "msedgedriver.exe")
# Maximize window
driver.maximize_window()
# Connect to Google
driver.get("https://www.google.com/")
driver.save_screenshot("screenshot.png")
driver.close()
```

## PTT Stock
```{python}
#| column: page
#| eval: false
from selenium import webdriver
from selenium.webdriver.common.by import By
from msedge.selenium_tools import Edge
# Path to Driver
driver = Edge(executable_path = "msedgedriver.exe")
# Connect PTT Stock
driver.get("https://www.ptt.cc/bbs/Stock/index.html")
# Html page source
# print(driver.page_source)
# Class Attribute
tags = driver.find_elements(By.CLASS_NAME,"title")
for tag in tags:
  print(tag.text)
# Get the previous titles
link = driver.find_element(By.LINK_TEXT,"‹ 上頁")
# Simulate clicking
link.click()
tags = driver.find_elements(By.CLASS_NAME,"title")
for tag in tags:
  print(tag.text)
driver.close()
```

## LinkedIn
```{python}
#| column: page
#| eval: false
from selenium import webdriver
from selenium.webdriver.common.by import By
from msedge.selenium_tools import Edge
import time
# Path to Driver
driver = Edge(executable_path = "msedgedriver.exe")
# Connect to LinkedIn
driver.get("https://www.linkedin.com/jobs/search?src=go-pa&trk=sem-ga_campid.18853522261_asid.146084015209_crid.633923221414_kw.linkedin_d.c_tid.kwd-296170574619_n.g_mt.e_geo.9031971&mcid=6994434350142418944&cid=&gclid=EAIaIQobChMI94CEo9Lu_QIVFRatBh0z1gK4EAAYASABEgJ1J_D_BwE&gclsrc=aw.ds&position=1&pageNum=0")
# Scroll Websites to the most bottom. Simulate with JavaScript
n = 0
while n < 3:
  driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
  # Wait for 3 seconds
  time.sleep(3)
  n+=1
# Job Titles
titleTags = driver.find_elements(By.CLASS_NAME, "base-search-card__title")
# For Loop
for titleTag in titleTags:
  print(titleTag.text)
driver.close()
```

## LeetCode
```{python}
#| column: page
#| eval: false
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from msedge.selenium_tools import Edge
import time
# Path to Driver
driver = Edge(executable_path = "msedgedriver.exe")
# Connect to LeetCode
driver.get("https://leetcode.com/accounts/login/")
# Enter Username and password
usernameInput = driver.find_element(By.ID, "id_login")
passwordInput = driver.find_element(By.ID, "id_password")
# Send information
usernameInput.send_keys("billy2003live@gmail.com")
passwordInput.send_keys("Me@0972716677")
signinBtn = driver.find_element(By.ID,"signin_btn")
signinBtn.send_keys(Keys.ENTER)
# Wait for 5 seconds
time.sleep(5)
# Access to information
driver.get("https://leetcode.com/problemset/all/")
time.sleep(3)
statElement = driver.find_element(By.CSS_SELECTOR,"[data-difficulty=TOTAL]")
# print(statElement.text)
columns = statElement.text.split("\n")
print(columns[1])
driver.close()
```

## Flask
```{python}
#| column: page
#| eval: false
from flask import Flask
# __name__ current module
app = Flask(__name__)
# Decorator: 以函式為基礎，提供附加功能
# root 
@app.route("/")
def home():
    return "Hello Flask"
# how to deal with /test
@app.route("/test")
def test():
    return "This is Test"

if __name__ == "__main__":
  app.run()
```




# R Web Crawler

## Star Wars films
```{r}
#| column: page
html <- read_html("https://rvest.tidyverse.org/articles/starwars.html")
```

```{r}
#| column: page
tibble(
  Title = html |> 
          html_elements("section") |> 
          html_element("h2") |> 
          html_text2(),
  `Released Date` = html |> 
                     html_elements("section") |> 
                     html_element("p") |> 
                     html_text2() |> 
                     str_remove("Released: ") |> 
                     parse_date(),
  Director = html |> 
             html_elements("section") |> 
             html_element(".director") |> 
             html_text2(),
  Intro = html |> 
          html_elements("section") |> 
          html_element(".crawl") |> 
          html_text2()
) |> 
  gt() |> 
  cols_align(columns = everything(),align = "center")
```

## PTT Movies
```{r}
#| column: page
moveis_html <- read_html("https://www.ptt.cc/bbs/movie/index.html")
```

```{r}
#| column: page
moveis_html |> 
           html_elements("div") |> 
           html_elements(".title") |> 
           html_text2()
```

##  IMDB top films
```{r}
#| column: page
imdb_url <- read_html("https://www.imdb.com/chart/top")
```

```{r}
#| column: page
imdb_url |> 
         html_element("table") |> 
         html_table() |> 
         select(
             rank_title_year = `Rank & Title`,
             rating = `IMDb Rating`
                ) |> 
         mutate(
            rank_title_year = str_replace_all(rank_title_year, "\n +", " ")
               ) |> 
        separate_wider_regex(
            rank_title_year,
              patterns = c(
               rank = "\\d+", "\\. ",
               title = ".+", " +\\(",
               year = "\\d+", "\\)")
         ) |> 
         reactable(resizable = T,filterable = T,defaultColDef = colDef(align = "center"))
```
